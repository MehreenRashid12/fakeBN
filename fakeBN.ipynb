{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0m9rGN1ZO5m"
      },
      "source": [
        "## Installing and Importing Packages ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38bYx54lP2sO"
      },
      "outputs": [],
      "source": [
        "!pip install pybbn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEj6KOA5t1S8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_validate, RepeatedStratifiedKFold\n",
        "\n",
        "from pybbn.graph.dag import Bbn\n",
        "from pybbn.graph.edge import Edge, EdgeType\n",
        "from pybbn.graph.jointree import EvidenceBuilder\n",
        "from pybbn.graph.node import BbnNode\n",
        "from pybbn.graph.variable import Variable\n",
        "from pybbn.pptc.inferencecontroller import InferenceController"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbJUXBIxC-zr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4WBKtapCwyU"
      },
      "source": [
        "## Final Dataset Info##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrP8n92qC09S"
      },
      "outputs": [],
      "source": [
        "final_data = pd.read_csv('/content/drive/MyDrive/fakeBN/data/final_data.csv')\n",
        "final_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZoLFAopK4JK"
      },
      "outputs": [],
      "source": [
        "# Count rows where majority_target is 1\n",
        "count_majority_1 = final_data[final_data['majority_target'] == 1].shape[0]\n",
        "\n",
        "# Count rows where majority_target is 0\n",
        "count_majority_0 = final_data[final_data['majority_target'] == 0].shape[0]\n",
        "\n",
        "print(\"Total true news :\", count_majority_1)\n",
        "print(\"Total fake news :\", count_majority_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyZVGYDPF5xh"
      },
      "outputs": [],
      "source": [
        "# Find the earliest and latest timestamps\n",
        "earliest_timestamp = final_data['timestamp'].min()\n",
        "latest_timestamp = final_data['timestamp'].max()\n",
        "\n",
        "print(\"Earliest Timestamp :\", earliest_timestamp)\n",
        "print(\"Latest Timestamp :\", latest_timestamp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txYiD13PCr6g"
      },
      "source": [
        "## Fake News Detection with BN ##\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7m07R6jC2QL"
      },
      "outputs": [],
      "source": [
        "data_columns = [\n",
        "    'majority_target', 'followers_count', 'friends_count', 'favourites_count', 'statuses_count', 'following', 'mentions',\n",
        "    'quotes', 'replies', 'retweets', 'favourites', 'hashtags', 'URLs', 'BotScoreBinary', 'cred', 'normalize_influence',\n",
        "    'unique_count', 'total_count', 'ORG_percentage', 'NORP_percentage', 'GPE_percentage', 'PERSON_percentage',\n",
        "    'MONEY_percentage', 'DATE_percentage', 'CARDINAL_percentage', 'PERCENT_percentage', 'ORDINAL_percentage',\n",
        "    'FAC_percentage', 'LAW_percentage', 'PRODUCT_percentage', 'EVENT_percentage', 'TIME_percentage', 'LOC_percentage',\n",
        "    'WORK_OF_ART_percentage', 'QUANTITY_percentage', 'LANGUAGE_percentage', 'Word count', 'Max word length',\n",
        "    'Min word length', 'Average word length', 'present_verbs', 'past_verbs', 'adjectives', 'adverbs', 'adpositions',\n",
        "    'pronouns', 'TOs', 'determiners', 'conjunctions', 'dots', 'exclamation', 'questions', 'ampersand', 'capitals',\n",
        "    'digits', 'long_word_freq', 'short_word_freq', 'matches_significant_event', 'day', 'hour', 'minute', 'month', 'quarter',\n",
        "    'is_weekend']\n",
        "who_columns = ['majority_target',  'followers_count', 'friends_count', 'favourites_count' , 'statuses_count', 'following',\n",
        "               'mentions', 'quotes', 'replies', 'retweets', 'favourites', 'hashtags', 'URLs', 'BotScoreBinary', 'cred', 'normalize_influence']\n",
        "when_columns = ['majority_target', 'matches_significant_event', 'day', 'hour', 'minute', 'month', 'quarter', 'is_weekend']\n",
        "\n",
        "what_columns = ['majority_target', 'present_verbs', 'past_verbs', 'adjectives', 'adverbs', 'adpositions', 'pronouns', 'TOs', 'determiners', 'conjunctions',\n",
        "                'dots', 'exclamation', 'questions', 'ampersand', 'capitals', 'digits', 'long_word_freq', 'short_word_freq', 'unique_count', 'total_count', 'ORG_percentage', 'NORP_percentage', 'GPE_percentage', 'PERSON_percentage',\n",
        "                'MONEY_percentage', 'DATE_percentage', 'CARDINAL_percentage', 'PERCENT_percentage', 'ORDINAL_percentage', 'FAC_percentage',\n",
        "                'LAW_percentage', 'PRODUCT_percentage', 'EVENT_percentage', 'TIME_percentage', 'LOC_percentage', 'WORK_OF_ART_percentage',\n",
        "                'QUANTITY_percentage', 'LANGUAGE_percentage', 'Word count', 'Max word length', 'Min word length', 'Average word length']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# only in case of uniform feature experiment\n",
        "\n",
        "# who_columns = who_columns[:8]\n",
        "# what_columns = what_columns[:8]\n",
        "# when_columns = when_columns[:8]"
      ],
      "metadata": {
        "id": "s0AC-zJV1pDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHWwy8SlMAnR"
      },
      "outputs": [],
      "source": [
        "final_data = pd.read_csv('/content/drive/MyDrive/wwwBN/data/final_data.csv')\n",
        "data = final_data[data_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xm_aqL5NQKEa"
      },
      "outputs": [],
      "source": [
        "# who, what and when modules\n",
        "\n",
        "# Initialize lists to store metrics\n",
        "who_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "what_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "when_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "bn_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "for train_index, test_index in kf.split(data, data['majority_target']):\n",
        "    np.random.shuffle(train_index)\n",
        "    midpoint = len(train_index) // 2\n",
        "    module_training = train_index[:midpoint]\n",
        "    cpt_formation = train_index[midpoint:]\n",
        "    module_training_data = data.iloc[module_training].copy().reset_index(drop=True)\n",
        "    cpt_data = data.iloc[cpt_formation].copy().reset_index(drop=True)\n",
        "    test_data = data.iloc[test_index].copy().reset_index(drop=True)\n",
        "\n",
        "    # Module training\n",
        "    who_mt = module_training_data[who_columns].copy()\n",
        "    what_mt = module_training_data[what_columns].copy()\n",
        "    when_mt = module_training_data[when_columns].copy()\n",
        "\n",
        "    who_mt_attr = who_mt.drop('majority_target', axis=1)\n",
        "    what_mt_attr = what_mt.drop('majority_target', axis=1)\n",
        "    when_mt_attr = when_mt.drop('majority_target', axis=1)\n",
        "\n",
        "    who_mt_label = who_mt['majority_target']\n",
        "    what_mt_label = what_mt['majority_target']\n",
        "    when_mt_label = when_mt['majority_target']\n",
        "\n",
        "    # Training and evaluating the who model\n",
        "    X_train, X_test, y_train, y_test = train_test_split(who_mt_attr, who_mt_label, test_size=0.2, random_state=42)\n",
        "    who_model = RandomForestClassifier()\n",
        "    who_model.fit(X_train, y_train)\n",
        "    predictions = who_model.predict(X_test)\n",
        "    who_metrics['accuracy'].append(accuracy_score(y_test, predictions))\n",
        "    who_metrics['precision'].append(precision_score(y_test, predictions))\n",
        "    who_metrics['recall'].append(recall_score(y_test, predictions))\n",
        "    who_metrics['f1'].append(f1_score(y_test, predictions))\n",
        "    print(f'Who model - Accuracy: {who_metrics[\"accuracy\"][-1]}, Precision: {who_metrics[\"precision\"][-1]}, Recall: {who_metrics[\"recall\"][-1]}, F1: {who_metrics[\"f1\"][-1]}')\n",
        "\n",
        "    # Training and evaluating the what model\n",
        "    X_train, X_test, y_train, y_test = train_test_split(what_mt_attr, what_mt_label, test_size=0.2, random_state=42)\n",
        "    what_model = RandomForestClassifier()\n",
        "    what_model.fit(X_train, y_train)\n",
        "    predictions = what_model.predict(X_test)\n",
        "    what_metrics['accuracy'].append(accuracy_score(y_test, predictions))\n",
        "    what_metrics['precision'].append(precision_score(y_test, predictions))\n",
        "    what_metrics['recall'].append(recall_score(y_test, predictions))\n",
        "    what_metrics['f1'].append(f1_score(y_test, predictions))\n",
        "    print(f'What model - Accuracy: {what_metrics[\"accuracy\"][-1]}, Precision: {what_metrics[\"precision\"][-1]}, Recall: {what_metrics[\"recall\"][-1]}, F1: {what_metrics[\"f1\"][-1]}')\n",
        "\n",
        "    # Training and evaluating the when model\n",
        "    X_train, X_test, y_train, y_test = train_test_split(when_mt_attr, when_mt_label, test_size=0.2, random_state=42)\n",
        "    when_model = RandomForestClassifier()\n",
        "    when_model.fit(X_train, y_train)\n",
        "    predictions = when_model.predict(X_test)\n",
        "    when_metrics['accuracy'].append(accuracy_score(y_test, predictions))\n",
        "    when_metrics['precision'].append(precision_score(y_test, predictions))\n",
        "    when_metrics['recall'].append(recall_score(y_test, predictions))\n",
        "    when_metrics['f1'].append(f1_score(y_test, predictions))\n",
        "    print(f'When model - Accuracy: {when_metrics[\"accuracy\"][-1]}, Precision: {when_metrics[\"precision\"][-1]}, Recall: {when_metrics[\"recall\"][-1]}, F1: {when_metrics[\"f1\"][-1]}')\n",
        "\n",
        "    # CPT formation\n",
        "    who_cp = cpt_data[who_columns].copy()\n",
        "    what_cp = cpt_data[what_columns].copy()\n",
        "    when_cp = cpt_data[when_columns].copy()\n",
        "\n",
        "    who_cp_attr = who_cp.drop('majority_target', axis=1)\n",
        "    what_cp_attr = what_cp.drop('majority_target', axis=1)\n",
        "    when_cp_attr = when_cp.drop('majority_target', axis=1)\n",
        "\n",
        "    who_cp_label = who_cp['majority_target']\n",
        "    what_cp_label = what_cp['majority_target']\n",
        "    when_cp_label = when_cp['majority_target']\n",
        "\n",
        "    who_cp_pred = who_model.predict(who_cp_attr)\n",
        "    what_cp_pred = what_model.predict(what_cp_attr)\n",
        "    when_cp_pred = when_model.predict(when_cp_attr)\n",
        "\n",
        "    total_true_cp = np.sum(cpt_data['majority_target'])\n",
        "    total_fake_cp = len(cpt_data) - np.sum(cpt_data['majority_target'])\n",
        "\n",
        "    who_t_t = who_t_f = who_f_t = who_f_f = 0\n",
        "    what_t_t = what_t_f = what_f_t = what_f_f = 0\n",
        "    when_t_t = when_t_f = when_f_t = when_f_f = 0\n",
        "\n",
        "    for i in range(len(cpt_data)):\n",
        "        if who_cp_pred[i] == 1 and who_cp_label[i] == 1:\n",
        "            who_t_t += 1\n",
        "        if who_cp_pred[i] == 1 and who_cp_label[i] == 0:\n",
        "            who_t_f += 1\n",
        "        if who_cp_pred[i] == 0 and who_cp_label[i] == 1:\n",
        "            who_f_t += 1\n",
        "        if who_cp_pred[i] == 0 and who_cp_label[i] == 0:\n",
        "            who_f_f += 1\n",
        "\n",
        "        if what_cp_pred[i] == 1 and what_cp_label[i] == 1:\n",
        "            what_t_t += 1\n",
        "        if what_cp_pred[i] == 1 and what_cp_label[i] == 0:\n",
        "            what_t_f += 1\n",
        "        if what_cp_pred[i] == 0 and what_cp_label[i] == 1:\n",
        "            what_f_t += 1\n",
        "        if what_cp_pred[i] == 0 and what_cp_label[i] == 0:\n",
        "            what_f_f += 1\n",
        "\n",
        "        if when_cp_pred[i] == 1 and when_cp_label[i] == 1:\n",
        "            when_t_t += 1\n",
        "        if when_cp_pred[i] == 1 and when_cp_label[i] == 0:\n",
        "            when_t_f += 1\n",
        "        if when_cp_pred[i] == 0 and when_cp_label[i] == 1:\n",
        "            when_f_t += 1\n",
        "        if when_cp_pred[i] == 0 and when_cp_label[i] == 0:\n",
        "            when_f_f += 1\n",
        "\n",
        "    prob_who_t_t = who_t_t / total_true_cp\n",
        "    prob_who_t_f = who_t_f / total_fake_cp\n",
        "    prob_who_f_t = who_f_t / total_true_cp\n",
        "    prob_who_f_f = who_f_f / total_fake_cp\n",
        "\n",
        "    prob_what_t_t = what_t_t / total_true_cp\n",
        "    prob_what_t_f = what_t_f / total_fake_cp\n",
        "    prob_what_f_t = what_f_t / total_true_cp\n",
        "    prob_what_f_f = what_f_f / total_fake_cp\n",
        "\n",
        "    prob_when_t_t = when_t_t / total_true_cp\n",
        "    prob_when_t_f = when_t_f / total_fake_cp\n",
        "    prob_when_f_t = when_f_t / total_true_cp\n",
        "    prob_when_f_f = when_f_f / total_fake_cp\n",
        "\n",
        "    prob_t = total_true_cp / (total_true_cp + total_fake_cp)\n",
        "    prob_f = total_fake_cp / (total_true_cp + total_fake_cp)\n",
        "\n",
        "    # Bayesian network\n",
        "    a = BbnNode(Variable(0, 'Authenticity', ['1', '0']), [prob_t, prob_f])\n",
        "    who = BbnNode(Variable(1, 'Who', ['1', '0']), [prob_who_t_t, prob_who_f_t, prob_who_t_f, prob_who_f_f])\n",
        "    what = BbnNode(Variable(2, 'What', ['1', '0']), [prob_what_t_t, prob_what_f_t, prob_what_t_f, prob_what_f_f])\n",
        "    when = BbnNode(Variable(3, 'When', ['1', '0']), [prob_when_t_t, prob_when_f_t, prob_when_t_f, prob_when_f_f])\n",
        "\n",
        "    bbn = Bbn() \\\n",
        "        .add_node(a) \\\n",
        "        .add_node(who) \\\n",
        "        .add_node(what) \\\n",
        "        .add_node(when) \\\n",
        "        .add_edge(Edge(a, who, EdgeType.DIRECTED)) \\\n",
        "        .add_edge(Edge(a, what, EdgeType.DIRECTED)) \\\n",
        "        .add_edge(Edge(a, when, EdgeType.DIRECTED))\n",
        "\n",
        "    join_tree = InferenceController.apply(bbn)\n",
        "\n",
        "    # Testing\n",
        "    who_test = test_data[who_columns].copy()\n",
        "    what_test = test_data[what_columns].copy()\n",
        "    when_test = test_data[when_columns].copy()\n",
        "\n",
        "    who_test_attr = who_test.drop('majority_target', axis=1)\n",
        "    what_test_attr = what_test.drop('majority_target', axis=1)\n",
        "    when_test_attr = when_test.drop('majority_target', axis=1)\n",
        "\n",
        "    test_label = test_data['majority_target']\n",
        "\n",
        "    who_test_pred = who_model.predict(who_test_attr)\n",
        "    what_test_pred = what_model.predict(what_test_attr)\n",
        "    when_test_pred = when_model.predict(when_test_attr)\n",
        "\n",
        "    who_test_pred = who_test_pred.astype(int)\n",
        "    what_test_pred = what_test_pred.astype(int)\n",
        "    when_test_pred = when_test_pred.astype(int)\n",
        "\n",
        "    bn_predictions = []\n",
        "    for i in range(len(test_data)):\n",
        "        join_tree = InferenceController.apply(bbn)\n",
        "        who_ev = str(who_test_pred[i])\n",
        "        what_ev = str(what_test_pred[i])\n",
        "        when_ev = str(when_test_pred[i])\n",
        "\n",
        "        ev1 = EvidenceBuilder() \\\n",
        "            .with_node(join_tree.get_bbn_node_by_name('Who')) \\\n",
        "            .with_evidence(who_ev, 1.0) \\\n",
        "            .build()\n",
        "        join_tree.set_observation(ev1)\n",
        "\n",
        "        ev2 = EvidenceBuilder() \\\n",
        "            .with_node(join_tree.get_bbn_node_by_name('What')) \\\n",
        "            .with_evidence(what_ev, 1.0) \\\n",
        "            .build()\n",
        "        join_tree.set_observation(ev2)\n",
        "\n",
        "        ev3 = EvidenceBuilder() \\\n",
        "            .with_node(join_tree.get_bbn_node_by_name('When')) \\\n",
        "            .with_evidence(when_ev, 1.0) \\\n",
        "            .build()\n",
        "        join_tree.set_observation(ev3)\n",
        "\n",
        "        query_node = join_tree.get_bbn_node_by_name('Authenticity')\n",
        "        potential = join_tree.get_bbn_potential(query_node)\n",
        "\n",
        "        for node, posteriors in join_tree.get_posteriors().items():\n",
        "            if node == 'Authenticity':\n",
        "                if posteriors['1'] > posteriors['0']:\n",
        "                    predicted_label = 1\n",
        "                else:\n",
        "                    predicted_label = 0\n",
        "                bn_predictions.append(predicted_label)\n",
        "                break\n",
        "\n",
        "    bn_metrics['accuracy'].append(accuracy_score(test_label, bn_predictions))\n",
        "    bn_metrics['precision'].append(precision_score(test_label, bn_predictions))\n",
        "    bn_metrics['recall'].append(recall_score(test_label, bn_predictions))\n",
        "    bn_metrics['f1'].append(f1_score(test_label, bn_predictions))\n",
        "\n",
        "    print(f'BN model - Accuracy: {bn_metrics[\"accuracy\"][-1]}, Precision: {bn_metrics[\"precision\"][-1]}, Recall: {bn_metrics[\"recall\"][-1]}, F1: {bn_metrics[\"f1\"][-1]}')\n",
        "    print(\"\\n ----------------------------------------------------------------------------- \\n\")\n",
        "# Calculate and print average metrics for each model\n",
        "print(\"\\nAverage Metrics:\")\n",
        "for model_name, metrics in [('Who', who_metrics), ('What', what_metrics), ('When', when_metrics), ('BN', bn_metrics)]:\n",
        "    print(f'\\n{model_name} model:')\n",
        "    for metric_name, metric_values in metrics.items():\n",
        "        avg_metric = np.mean(metric_values)\n",
        "        print(f'  Average {metric_name}: {avg_metric:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAD0z0djuXk3"
      },
      "outputs": [],
      "source": [
        "# who and what module\n",
        "\n",
        "# Initialize lists to store metrics\n",
        "who_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "what_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "bn_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "for train_index, test_index in kf.split(data, data['majority_target']):\n",
        "    np.random.shuffle(train_index)\n",
        "    midpoint = len(train_index) // 2\n",
        "    module_training = train_index[:midpoint]\n",
        "    cpt_formation = train_index[midpoint:]\n",
        "    module_training_data = data.iloc[module_training].copy().reset_index(drop=True)\n",
        "    cpt_data = data.iloc[cpt_formation].copy().reset_index(drop=True)\n",
        "    test_data = data.iloc[test_index].copy().reset_index(drop=True)\n",
        "\n",
        "    # Module training\n",
        "    who_mt = module_training_data[who_columns].copy()\n",
        "    what_mt = module_training_data[what_columns].copy()\n",
        "\n",
        "    who_mt_attr = who_mt.drop('majority_target', axis=1)\n",
        "    what_mt_attr = what_mt.drop('majority_target', axis=1)\n",
        "\n",
        "    who_mt_label = who_mt['majority_target']\n",
        "    what_mt_label = what_mt['majority_target']\n",
        "\n",
        "    # Training and evaluating the who model\n",
        "    X_train, X_test, y_train, y_test = train_test_split(who_mt_attr, who_mt_label, test_size=0.2, random_state=42)\n",
        "    who_model = RandomForestClassifier()\n",
        "    who_model.fit(X_train, y_train)\n",
        "    predictions = who_model.predict(X_test)\n",
        "    who_metrics['accuracy'].append(accuracy_score(y_test, predictions))\n",
        "    who_metrics['precision'].append(precision_score(y_test, predictions))\n",
        "    who_metrics['recall'].append(recall_score(y_test, predictions))\n",
        "    who_metrics['f1'].append(f1_score(y_test, predictions))\n",
        "    print(f'Who model - Accuracy: {who_metrics[\"accuracy\"][-1]}, Precision: {who_metrics[\"precision\"][-1]}, Recall: {who_metrics[\"recall\"][-1]}, F1: {who_metrics[\"f1\"][-1]}')\n",
        "\n",
        "    # Training and evaluating the what model\n",
        "    X_train, X_test, y_train, y_test = train_test_split(what_mt_attr, what_mt_label, test_size=0.2, random_state=42)\n",
        "    what_model = RandomForestClassifier()\n",
        "    what_model.fit(X_train, y_train)\n",
        "    predictions = what_model.predict(X_test)\n",
        "    what_metrics['accuracy'].append(accuracy_score(y_test, predictions))\n",
        "    what_metrics['precision'].append(precision_score(y_test, predictions))\n",
        "    what_metrics['recall'].append(recall_score(y_test, predictions))\n",
        "    what_metrics['f1'].append(f1_score(y_test, predictions))\n",
        "    print(f'What model - Accuracy: {what_metrics[\"accuracy\"][-1]}, Precision: {what_metrics[\"precision\"][-1]}, Recall: {what_metrics[\"recall\"][-1]}, F1: {what_metrics[\"f1\"][-1]}')\n",
        "\n",
        "    # CPT formation\n",
        "    who_cp = cpt_data[who_columns].copy()\n",
        "    what_cp = cpt_data[what_columns].copy()\n",
        "\n",
        "    who_cp_attr = who_cp.drop('majority_target', axis=1)\n",
        "    what_cp_attr = what_cp.drop('majority_target', axis=1)\n",
        "\n",
        "    who_cp_label = who_cp['majority_target']\n",
        "    what_cp_label = what_cp['majority_target']\n",
        "\n",
        "    who_cp_pred = who_model.predict(who_cp_attr)\n",
        "    what_cp_pred = what_model.predict(what_cp_attr)\n",
        "\n",
        "    total_true_cp = np.sum(cpt_data['majority_target'])\n",
        "    total_fake_cp = len(cpt_data) - np.sum(cpt_data['majority_target'])\n",
        "\n",
        "    who_t_t = who_t_f = who_f_t = who_f_f = 0\n",
        "    what_t_t = what_t_f = what_f_t = what_f_f = 0\n",
        "\n",
        "    for i in range(len(cpt_data)):\n",
        "        if who_cp_pred[i] == 1 and who_cp_label[i] == 1:\n",
        "            who_t_t += 1\n",
        "        if who_cp_pred[i] == 1 and who_cp_label[i] == 0:\n",
        "            who_t_f += 1\n",
        "        if who_cp_pred[i] == 0 and who_cp_label[i] == 1:\n",
        "            who_f_t += 1\n",
        "        if who_cp_pred[i] == 0 and who_cp_label[i] == 0:\n",
        "            who_f_f += 1\n",
        "\n",
        "        if what_cp_pred[i] == 1 and what_cp_label[i] == 1:\n",
        "            what_t_t += 1\n",
        "        if what_cp_pred[i] == 1 and what_cp_label[i] == 0:\n",
        "            what_t_f += 1\n",
        "        if what_cp_pred[i] == 0 and what_cp_label[i] == 1:\n",
        "            what_f_t += 1\n",
        "        if what_cp_pred[i] == 0 and what_cp_label[i] == 0:\n",
        "            what_f_f += 1\n",
        "\n",
        "    prob_who_t_t = who_t_t / total_true_cp\n",
        "    prob_who_t_f = who_t_f / total_fake_cp\n",
        "    prob_who_f_t = who_f_t / total_true_cp\n",
        "    prob_who_f_f = who_f_f / total_fake_cp\n",
        "\n",
        "    prob_what_t_t = what_t_t / total_true_cp\n",
        "    prob_what_t_f = what_t_f / total_fake_cp\n",
        "    prob_what_f_t = what_f_t / total_true_cp\n",
        "    prob_what_f_f = what_f_f / total_fake_cp\n",
        "\n",
        "    prob_t = total_true_cp / (total_true_cp + total_fake_cp)\n",
        "    prob_f = total_fake_cp / (total_true_cp + total_fake_cp)\n",
        "\n",
        "    # Bayesian network\n",
        "    a = BbnNode(Variable(0, 'Authenticity', ['1', '0']), [prob_t, prob_f])\n",
        "    who = BbnNode(Variable(1, 'Who', ['1', '0']), [prob_who_t_t, prob_who_f_t, prob_who_t_f, prob_who_f_f])\n",
        "    what = BbnNode(Variable(2, 'What', ['1', '0']), [prob_what_t_t, prob_what_f_t, prob_what_t_f, prob_what_f_f])\n",
        "\n",
        "    bbn = Bbn() \\\n",
        "        .add_node(a) \\\n",
        "        .add_node(who) \\\n",
        "        .add_node(what) \\\n",
        "        .add_edge(Edge(a, who, EdgeType.DIRECTED)) \\\n",
        "        .add_edge(Edge(a, what, EdgeType.DIRECTED))\n",
        "\n",
        "    join_tree = InferenceController.apply(bbn)\n",
        "\n",
        "    # Testing\n",
        "    who_test = test_data[who_columns].copy()\n",
        "    what_test = test_data[what_columns].copy()\n",
        "\n",
        "    who_test_attr = who_test.drop('majority_target', axis=1)\n",
        "    what_test_attr = what_test.drop('majority_target', axis=1)\n",
        "\n",
        "    test_label = test_data['majority_target']\n",
        "\n",
        "    who_test_pred = who_model.predict(who_test_attr)\n",
        "    what_test_pred = what_model.predict(what_test_attr)\n",
        "\n",
        "    who_test_pred = who_test_pred.astype(int)\n",
        "    what_test_pred = what_test_pred.astype(int)\n",
        "\n",
        "    bn_predictions = []\n",
        "    for i in range(len(test_data)):\n",
        "        join_tree = InferenceController.apply(bbn)\n",
        "        who_ev = str(who_test_pred[i])\n",
        "        what_ev = str(what_test_pred[i])\n",
        "\n",
        "        ev1 = EvidenceBuilder() \\\n",
        "            .with_node(join_tree.get_bbn_node_by_name('Who')) \\\n",
        "            .with_evidence(who_ev, 1.0) \\\n",
        "            .build()\n",
        "        join_tree.set_observation(ev1)\n",
        "\n",
        "        ev2 = EvidenceBuilder() \\\n",
        "            .with_node(join_tree.get_bbn_node_by_name('What')) \\\n",
        "            .with_evidence(what_ev, 1.0) \\\n",
        "            .build()\n",
        "        join_tree.set_observation(ev2)\n",
        "\n",
        "        query_node = join_tree.get_bbn_node_by_name('Authenticity')\n",
        "        potential = join_tree.get_bbn_potential(query_node)\n",
        "\n",
        "        for node, posteriors in join_tree.get_posteriors().items():\n",
        "            if node == 'Authenticity':\n",
        "                if posteriors['1'] > posteriors['0']:\n",
        "                    predicted_label = 1\n",
        "                else:\n",
        "                    predicted_label = 0\n",
        "                bn_predictions.append(predicted_label)\n",
        "                break\n",
        "\n",
        "    bn_metrics['accuracy'].append(accuracy_score(test_label, bn_predictions))\n",
        "    bn_metrics['precision'].append(precision_score(test_label, bn_predictions))\n",
        "    bn_metrics['recall'].append(recall_score(test_label, bn_predictions))\n",
        "    bn_metrics['f1'].append(f1_score(test_label, bn_predictions))\n",
        "\n",
        "    print(f'BN model - Accuracy: {bn_metrics[\"accuracy\"][-1]}, Precision: {bn_metrics[\"precision\"][-1]}, Recall: {bn_metrics[\"recall\"][-1]}, F1: {bn_metrics[\"f1\"][-1]}')\n",
        "    print(\"\\n ----------------------------------------------------------------------------- \\n\")\n",
        "\n",
        "# Calculate and print average metrics for each model\n",
        "print(\"\\nAverage Metrics:\")\n",
        "for model_name, metrics in [('Who', who_metrics), ('What', what_metrics), ('BN', bn_metrics)]:\n",
        "    print(f'\\n{model_name} model:')\n",
        "    for metric_name, metric_values in metrics.items():\n",
        "        avg_metric = np.mean(metric_values)\n",
        "        print(f'  Average {metric_name}: {avg_metric:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTku7TiEuXOF"
      },
      "outputs": [],
      "source": [
        "# who and when module\n",
        "\n",
        "# Initialize lists to store metrics\n",
        "who_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "when_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "bn_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "for train_index, test_index in kf.split(data, data['majority_target']):\n",
        "    np.random.shuffle(train_index)\n",
        "    midpoint = len(train_index) // 2\n",
        "    module_training = train_index[:midpoint]\n",
        "    cpt_formation = train_index[midpoint:]\n",
        "    module_training_data = data.iloc[module_training].copy().reset_index(drop=True)\n",
        "    cpt_data = data.iloc[cpt_formation].copy().reset_index(drop=True)\n",
        "    test_data = data.iloc[test_index].copy().reset_index(drop=True)\n",
        "\n",
        "    # Module training\n",
        "    who_mt = module_training_data[who_columns].copy()\n",
        "    when_mt = module_training_data[when_columns].copy()\n",
        "\n",
        "    who_mt_attr = who_mt.drop('majority_target', axis=1)\n",
        "    when_mt_attr = when_mt.drop('majority_target', axis=1)\n",
        "\n",
        "    who_mt_label = who_mt['majority_target']\n",
        "    when_mt_label = when_mt['majority_target']\n",
        "\n",
        "    # Training and evaluating the who model\n",
        "    X_train, X_test, y_train, y_test = train_test_split(who_mt_attr, who_mt_label, test_size=0.2, random_state=42)\n",
        "    who_model = RandomForestClassifier()\n",
        "    who_model.fit(X_train, y_train)\n",
        "    predictions = who_model.predict(X_test)\n",
        "    who_metrics['accuracy'].append(accuracy_score(y_test, predictions))\n",
        "    who_metrics['precision'].append(precision_score(y_test, predictions))\n",
        "    who_metrics['recall'].append(recall_score(y_test, predictions))\n",
        "    who_metrics['f1'].append(f1_score(y_test, predictions))\n",
        "    print(f'Who model - Accuracy: {who_metrics[\"accuracy\"][-1]}, Precision: {who_metrics[\"precision\"][-1]}, Recall: {who_metrics[\"recall\"][-1]}, F1: {who_metrics[\"f1\"][-1]}')\n",
        "\n",
        "    # Training and evaluating the when model\n",
        "    X_train, X_test, y_train, y_test = train_test_split(when_mt_attr, when_mt_label, test_size=0.2, random_state=42)\n",
        "    when_model = RandomForestClassifier()\n",
        "    when_model.fit(X_train, y_train)\n",
        "    predictions = when_model.predict(X_test)\n",
        "    when_metrics['accuracy'].append(accuracy_score(y_test, predictions))\n",
        "    when_metrics['precision'].append(precision_score(y_test, predictions))\n",
        "    when_metrics['recall'].append(recall_score(y_test, predictions))\n",
        "    when_metrics['f1'].append(f1_score(y_test, predictions))\n",
        "    print(f'When model - Accuracy: {when_metrics[\"accuracy\"][-1]}, Precision: {when_metrics[\"precision\"][-1]}, Recall: {when_metrics[\"recall\"][-1]}, F1: {when_metrics[\"f1\"][-1]}')\n",
        "\n",
        "    # CPT formation\n",
        "    who_cp = cpt_data[who_columns].copy()\n",
        "    when_cp = cpt_data[when_columns].copy()\n",
        "\n",
        "    who_cp_attr = who_cp.drop('majority_target', axis=1)\n",
        "    when_cp_attr = when_cp.drop('majority_target', axis=1)\n",
        "\n",
        "    who_cp_label = who_cp['majority_target']\n",
        "    when_cp_label = when_cp['majority_target']\n",
        "\n",
        "    who_cp_pred = who_model.predict(who_cp_attr)\n",
        "    when_cp_pred = when_model.predict(when_cp_attr)\n",
        "\n",
        "    total_true_cp = np.sum(cpt_data['majority_target'])\n",
        "    total_fake_cp = len(cpt_data) - np.sum(cpt_data['majority_target'])\n",
        "\n",
        "    who_t_t = who_t_f = who_f_t = who_f_f = 0\n",
        "    when_t_t = when_t_f = when_f_t = when_f_f = 0\n",
        "\n",
        "    for i in range(len(cpt_data)):\n",
        "        if who_cp_pred[i] == 1 and who_cp_label[i] == 1:\n",
        "            who_t_t += 1\n",
        "        if who_cp_pred[i] == 1 and who_cp_label[i] == 0:\n",
        "            who_t_f += 1\n",
        "        if who_cp_pred[i] == 0 and who_cp_label[i] == 1:\n",
        "            who_f_t += 1\n",
        "        if who_cp_pred[i] == 0 and who_cp_label[i] == 0:\n",
        "            who_f_f += 1\n",
        "\n",
        "        if when_cp_pred[i] == 1 and when_cp_label[i] == 1:\n",
        "            when_t_t += 1\n",
        "        if when_cp_pred[i] == 1 and when_cp_label[i] == 0:\n",
        "            when_t_f += 1\n",
        "        if when_cp_pred[i] == 0 and when_cp_label[i] == 1:\n",
        "            when_f_t += 1\n",
        "        if when_cp_pred[i] == 0 and when_cp_label[i] == 0:\n",
        "            when_f_f += 1\n",
        "\n",
        "    prob_who_t_t = who_t_t / total_true_cp\n",
        "    prob_who_t_f = who_t_f / total_fake_cp\n",
        "    prob_who_f_t = who_f_t / total_true_cp\n",
        "    prob_who_f_f = who_f_f / total_fake_cp\n",
        "\n",
        "    prob_when_t_t = when_t_t / total_true_cp\n",
        "    prob_when_t_f = when_t_f / total_fake_cp\n",
        "    prob_when_f_t = when_f_t / total_true_cp\n",
        "    prob_when_f_f = when_f_f / total_fake_cp\n",
        "\n",
        "    prob_t = total_true_cp / (total_true_cp + total_fake_cp)\n",
        "    prob_f = total_fake_cp / (total_true_cp + total_fake_cp)\n",
        "\n",
        "    # Bayesian network\n",
        "    a = BbnNode(Variable(0, 'Authenticity', ['1', '0']), [prob_t, prob_f])\n",
        "    who = BbnNode(Variable(1, 'Who', ['1', '0']), [prob_who_t_t, prob_who_f_t, prob_who_t_f, prob_who_f_f])\n",
        "    when = BbnNode(Variable(2, 'What', ['1', '0']), [prob_when_t_t, prob_when_f_t, prob_when_t_f, prob_when_f_f])\n",
        "\n",
        "    bbn = Bbn() \\\n",
        "        .add_node(a) \\\n",
        "        .add_node(who) \\\n",
        "        .add_node(when) \\\n",
        "        .add_edge(Edge(a, who, EdgeType.DIRECTED)) \\\n",
        "        .add_edge(Edge(a, when, EdgeType.DIRECTED))\n",
        "\n",
        "    join_tree = InferenceController.apply(bbn)\n",
        "\n",
        "    # Testing\n",
        "    who_test = test_data[who_columns].copy()\n",
        "    when_test = test_data[when_columns].copy()\n",
        "\n",
        "    who_test_attr = who_test.drop('majority_target', axis=1)\n",
        "    when_test_attr = when_test.drop('majority_target', axis=1)\n",
        "\n",
        "    test_label = test_data['majority_target']\n",
        "\n",
        "    who_test_pred = who_model.predict(who_test_attr)\n",
        "    when_test_pred = when_model.predict(when_test_attr)\n",
        "\n",
        "    who_test_pred = who_test_pred.astype(int)\n",
        "    when_test_pred = when_test_pred.astype(int)\n",
        "\n",
        "    bn_predictions = []\n",
        "    for i in range(len(test_data)):\n",
        "        join_tree = InferenceController.apply(bbn)\n",
        "        who_ev = str(who_test_pred[i])\n",
        "        when_ev = str(when_test_pred[i])\n",
        "\n",
        "        ev1 = EvidenceBuilder() \\\n",
        "            .with_node(join_tree.get_bbn_node_by_name('Who')) \\\n",
        "            .with_evidence(who_ev, 1.0) \\\n",
        "            .build()\n",
        "        join_tree.set_observation(ev1)\n",
        "\n",
        "        ev2 = EvidenceBuilder() \\\n",
        "            .with_node(join_tree.get_bbn_node_by_name('What')) \\\n",
        "            .with_evidence(when_ev, 1.0) \\\n",
        "            .build()\n",
        "        join_tree.set_observation(ev2)\n",
        "\n",
        "        query_node = join_tree.get_bbn_node_by_name('Authenticity')\n",
        "        potential = join_tree.get_bbn_potential(query_node)\n",
        "\n",
        "        for node, posteriors in join_tree.get_posteriors().items():\n",
        "            if node == 'Authenticity':\n",
        "                if posteriors['1'] > posteriors['0']:\n",
        "                    predicted_label = 1\n",
        "                else:\n",
        "                    predicted_label = 0\n",
        "                bn_predictions.append(predicted_label)\n",
        "                break\n",
        "\n",
        "    bn_metrics['accuracy'].append(accuracy_score(test_label, bn_predictions))\n",
        "    bn_metrics['precision'].append(precision_score(test_label, bn_predictions))\n",
        "    bn_metrics['recall'].append(recall_score(test_label, bn_predictions))\n",
        "    bn_metrics['f1'].append(f1_score(test_label, bn_predictions))\n",
        "\n",
        "    print(f'BN model - Accuracy: {bn_metrics[\"accuracy\"][-1]}, Precision: {bn_metrics[\"precision\"][-1]}, Recall: {bn_metrics[\"recall\"][-1]}, F1: {bn_metrics[\"f1\"][-1]}')\n",
        "    print(\"\\n ----------------------------------------------------------------------------- \\n\")\n",
        "\n",
        "# Calculate and print average metrics for each model\n",
        "print(\"\\nAverage Metrics:\")\n",
        "for model_name, metrics in [('Who', who_metrics), ('When', when_metrics), ('BN', bn_metrics)]:\n",
        "    print(f'\\n{model_name} model:')\n",
        "    for metric_name, metric_values in metrics.items():\n",
        "        avg_metric = np.mean(metric_values)\n",
        "        print(f'  Average {metric_name}: {avg_metric:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ngjn9oFgaVb"
      },
      "outputs": [],
      "source": [
        "# when and what module\n",
        "\n",
        "# Initialize lists to store metrics\n",
        "when_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "what_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "bn_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "for train_index, test_index in kf.split(data, data['majority_target']):\n",
        "    np.random.shuffle(train_index)\n",
        "    midpoint = len(train_index) // 2\n",
        "    module_training = train_index[:midpoint]\n",
        "    cpt_formation = train_index[midpoint:]\n",
        "    module_training_data = data.iloc[module_training].copy().reset_index(drop=True)\n",
        "    cpt_data = data.iloc[cpt_formation].copy().reset_index(drop=True)\n",
        "    test_data = data.iloc[test_index].copy().reset_index(drop=True)\n",
        "\n",
        "    # Module training\n",
        "    when_mt = module_training_data[when_columns].copy()\n",
        "    what_mt = module_training_data[what_columns].copy()\n",
        "\n",
        "    when_mt_attr = when_mt.drop('majority_target', axis=1)\n",
        "    what_mt_attr = what_mt.drop('majority_target', axis=1)\n",
        "\n",
        "    when_mt_label = when_mt['majority_target']\n",
        "    what_mt_label = what_mt['majority_target']\n",
        "\n",
        "    # Training and evaluating the when model\n",
        "    X_train, X_test, y_train, y_test = train_test_split(when_mt_attr, when_mt_label, test_size=0.2, random_state=42)\n",
        "    when_model = RandomForestClassifier()\n",
        "    when_model.fit(X_train, y_train)\n",
        "    predictions = when_model.predict(X_test)\n",
        "    when_metrics['accuracy'].append(accuracy_score(y_test, predictions))\n",
        "    when_metrics['precision'].append(precision_score(y_test, predictions))\n",
        "    when_metrics['recall'].append(recall_score(y_test, predictions))\n",
        "    when_metrics['f1'].append(f1_score(y_test, predictions))\n",
        "    print(f'When model - Accuracy: {when_metrics[\"accuracy\"][-1]}, Precision: {when_metrics[\"precision\"][-1]}, Recall: {when_metrics[\"recall\"][-1]}, F1: {when_metrics[\"f1\"][-1]}')\n",
        "\n",
        "    # Training and evaluating the what model\n",
        "    X_train, X_test, y_train, y_test = train_test_split(what_mt_attr, what_mt_label, test_size=0.2, random_state=42)\n",
        "    what_model = RandomForestClassifier()\n",
        "    what_model.fit(X_train, y_train)\n",
        "    predictions = what_model.predict(X_test)\n",
        "    what_metrics['accuracy'].append(accuracy_score(y_test, predictions))\n",
        "    what_metrics['precision'].append(precision_score(y_test, predictions))\n",
        "    what_metrics['recall'].append(recall_score(y_test, predictions))\n",
        "    what_metrics['f1'].append(f1_score(y_test, predictions))\n",
        "    print(f'What model - Accuracy: {what_metrics[\"accuracy\"][-1]}, Precision: {what_metrics[\"precision\"][-1]}, Recall: {what_metrics[\"recall\"][-1]}, F1: {what_metrics[\"f1\"][-1]}')\n",
        "\n",
        "    # CPT formation\n",
        "    when_cp = cpt_data[when_columns].copy()\n",
        "    what_cp = cpt_data[what_columns].copy()\n",
        "\n",
        "    when_cp_attr = when_cp.drop('majority_target', axis=1)\n",
        "    what_cp_attr = what_cp.drop('majority_target', axis=1)\n",
        "\n",
        "    when_cp_label = when_cp['majority_target']\n",
        "    what_cp_label = what_cp['majority_target']\n",
        "\n",
        "    when_cp_pred = when_model.predict(when_cp_attr)\n",
        "    what_cp_pred = what_model.predict(what_cp_attr)\n",
        "\n",
        "    total_true_cp = np.sum(cpt_data['majority_target'])\n",
        "    total_fake_cp = len(cpt_data) - np.sum(cpt_data['majority_target'])\n",
        "\n",
        "    when_t_t = when_t_f = when_f_t = when_f_f = 0\n",
        "    what_t_t = what_t_f = what_f_t = what_f_f = 0\n",
        "\n",
        "    for i in range(len(cpt_data)):\n",
        "        if when_cp_pred[i] == 1 and when_cp_label[i] == 1:\n",
        "            when_t_t += 1\n",
        "        if when_cp_pred[i] == 1 and when_cp_label[i] == 0:\n",
        "            when_t_f += 1\n",
        "        if when_cp_pred[i] == 0 and when_cp_label[i] == 1:\n",
        "            when_f_t += 1\n",
        "        if when_cp_pred[i] == 0 and when_cp_label[i] == 0:\n",
        "            when_f_f += 1\n",
        "\n",
        "        if what_cp_pred[i] == 1 and what_cp_label[i] == 1:\n",
        "            what_t_t += 1\n",
        "        if what_cp_pred[i] == 1 and what_cp_label[i] == 0:\n",
        "            what_t_f += 1\n",
        "        if what_cp_pred[i] == 0 and what_cp_label[i] == 1:\n",
        "            what_f_t += 1\n",
        "        if what_cp_pred[i] == 0 and what_cp_label[i] == 0:\n",
        "            what_f_f += 1\n",
        "\n",
        "    prob_when_t_t = when_t_t / total_true_cp\n",
        "    prob_when_t_f = when_t_f / total_fake_cp\n",
        "    prob_when_f_t = when_f_t / total_true_cp\n",
        "    prob_when_f_f = when_f_f / total_fake_cp\n",
        "\n",
        "    prob_what_t_t = what_t_t / total_true_cp\n",
        "    prob_what_t_f = what_t_f / total_fake_cp\n",
        "    prob_what_f_t = what_f_t / total_true_cp\n",
        "    prob_what_f_f = what_f_f / total_fake_cp\n",
        "\n",
        "    prob_t = total_true_cp / (total_true_cp + total_fake_cp)\n",
        "    prob_f = total_fake_cp / (total_true_cp + total_fake_cp)\n",
        "\n",
        "    # Bayesian network\n",
        "    a = BbnNode(Variable(0, 'Authenticity', ['1', '0']), [prob_t, prob_f])\n",
        "    when = BbnNode(Variable(1, 'Who', ['1', '0']), [prob_when_t_t, prob_when_f_t, prob_when_t_f, prob_when_f_f])\n",
        "    what = BbnNode(Variable(2, 'What', ['1', '0']), [prob_what_t_t, prob_what_f_t, prob_what_t_f, prob_what_f_f])\n",
        "\n",
        "    bbn = Bbn() \\\n",
        "        .add_node(a) \\\n",
        "        .add_node(when) \\\n",
        "        .add_node(what) \\\n",
        "        .add_edge(Edge(a, when, EdgeType.DIRECTED)) \\\n",
        "        .add_edge(Edge(a, what, EdgeType.DIRECTED))\n",
        "\n",
        "    join_tree = InferenceController.apply(bbn)\n",
        "\n",
        "    # Testing\n",
        "    when_test = test_data[when_columns].copy()\n",
        "    what_test = test_data[what_columns].copy()\n",
        "\n",
        "    when_test_attr = when_test.drop('majority_target', axis=1)\n",
        "    what_test_attr = what_test.drop('majority_target', axis=1)\n",
        "\n",
        "    test_label = test_data['majority_target']\n",
        "\n",
        "    when_test_pred = when_model.predict(when_test_attr)\n",
        "    what_test_pred = what_model.predict(what_test_attr)\n",
        "\n",
        "    when_test_pred = when_test_pred.astype(int)\n",
        "    what_test_pred = what_test_pred.astype(int)\n",
        "\n",
        "    bn_predictions = []\n",
        "    for i in range(len(test_data)):\n",
        "        join_tree = InferenceController.apply(bbn)\n",
        "        when_ev = str(when_test_pred[i])\n",
        "        what_ev = str(what_test_pred[i])\n",
        "\n",
        "        ev1 = EvidenceBuilder() \\\n",
        "            .with_node(join_tree.get_bbn_node_by_name('Who')) \\\n",
        "            .with_evidence(when_ev, 1.0) \\\n",
        "            .build()\n",
        "        join_tree.set_observation(ev1)\n",
        "\n",
        "        ev2 = EvidenceBuilder() \\\n",
        "            .with_node(join_tree.get_bbn_node_by_name('What')) \\\n",
        "            .with_evidence(what_ev, 1.0) \\\n",
        "            .build()\n",
        "        join_tree.set_observation(ev2)\n",
        "\n",
        "        query_node = join_tree.get_bbn_node_by_name('Authenticity')\n",
        "        potential = join_tree.get_bbn_potential(query_node)\n",
        "\n",
        "        for node, posteriors in join_tree.get_posteriors().items():\n",
        "            if node == 'Authenticity':\n",
        "                if posteriors['1'] > posteriors['0']:\n",
        "                    predicted_label = 1\n",
        "                else:\n",
        "                    predicted_label = 0\n",
        "                bn_predictions.append(predicted_label)\n",
        "                break\n",
        "\n",
        "    bn_metrics['accuracy'].append(accuracy_score(test_label, bn_predictions))\n",
        "    bn_metrics['precision'].append(precision_score(test_label, bn_predictions))\n",
        "    bn_metrics['recall'].append(recall_score(test_label, bn_predictions))\n",
        "    bn_metrics['f1'].append(f1_score(test_label, bn_predictions))\n",
        "\n",
        "    print(f'BN model - Accuracy: {bn_metrics[\"accuracy\"][-1]}, Precision: {bn_metrics[\"precision\"][-1]}, Recall: {bn_metrics[\"recall\"][-1]}, F1: {bn_metrics[\"f1\"][-1]}')\n",
        "    print(\"\\n ----------------------------------------------------------------------------- \\n\")\n",
        "\n",
        "# Calculate and print average metrics for each model\n",
        "print(\"\\nAverage Metrics:\")\n",
        "for model_name, metrics in [('When', when_metrics), ('What', what_metrics), ('BN', bn_metrics)]:\n",
        "    print(f'\\n{model_name} model:')\n",
        "    for metric_name, metric_values in metrics.items():\n",
        "        avg_metric = np.mean(metric_values)\n",
        "        print(f'  Average {metric_name}: {avg_metric:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# who module\n",
        "\n",
        "# Initialize lists to store metrics\n",
        "who_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "bn_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "for train_index, test_index in kf.split(data, data['majority_target']):\n",
        "    np.random.shuffle(train_index)\n",
        "    midpoint = len(train_index) // 2\n",
        "    module_training = train_index[:midpoint]\n",
        "    cpt_formation = train_index[midpoint:]\n",
        "    module_training_data = data.iloc[module_training].copy().reset_index(drop=True)\n",
        "    cpt_data = data.iloc[cpt_formation].copy().reset_index(drop=True)\n",
        "    test_data = data.iloc[test_index].copy().reset_index(drop=True)\n",
        "\n",
        "    # Module training\n",
        "    who_mt = module_training_data[who_columns].copy()\n",
        "\n",
        "    who_mt_attr = who_mt.drop('majority_target', axis=1)\n",
        "\n",
        "    who_mt_label = who_mt['majority_target']\n",
        "\n",
        "    # Training and evaluating the who model\n",
        "    X_train, X_test, y_train, y_test = train_test_split(who_mt_attr, who_mt_label, test_size=0.2, random_state=42)\n",
        "    who_model = RandomForestClassifier()\n",
        "    who_model.fit(X_train, y_train)\n",
        "    predictions = who_model.predict(X_test)\n",
        "    who_metrics['accuracy'].append(accuracy_score(y_test, predictions))\n",
        "    who_metrics['precision'].append(precision_score(y_test, predictions))\n",
        "    who_metrics['recall'].append(recall_score(y_test, predictions))\n",
        "    who_metrics['f1'].append(f1_score(y_test, predictions))\n",
        "    print(f'Who model - Accuracy: {who_metrics[\"accuracy\"][-1]}, Precision: {who_metrics[\"precision\"][-1]}, Recall: {who_metrics[\"recall\"][-1]}, F1: {who_metrics[\"f1\"][-1]}')\n",
        "\n",
        "    # CPT formation\n",
        "    who_cp = cpt_data[who_columns].copy()\n",
        "\n",
        "    who_cp_attr = who_cp.drop('majority_target', axis=1)\n",
        "\n",
        "    who_cp_label = who_cp['majority_target']\n",
        "\n",
        "    who_cp_pred = who_model.predict(who_cp_attr)\n",
        "\n",
        "    total_true_cp = np.sum(cpt_data['majority_target'])\n",
        "    total_fake_cp = len(cpt_data) - np.sum(cpt_data['majority_target'])\n",
        "\n",
        "    who_t_t = who_t_f = who_f_t = who_f_f = 0\n",
        "\n",
        "    for i in range(len(cpt_data)):\n",
        "        if who_cp_pred[i] == 1 and who_cp_label[i] == 1:\n",
        "            who_t_t += 1\n",
        "        if who_cp_pred[i] == 1 and who_cp_label[i] == 0:\n",
        "            who_t_f += 1\n",
        "        if who_cp_pred[i] == 0 and who_cp_label[i] == 1:\n",
        "            who_f_t += 1\n",
        "        if who_cp_pred[i] == 0 and who_cp_label[i] == 0:\n",
        "            who_f_f += 1\n",
        "\n",
        "    prob_who_t_t = who_t_t / total_true_cp\n",
        "    prob_who_t_f = who_t_f / total_fake_cp\n",
        "    prob_who_f_t = who_f_t / total_true_cp\n",
        "    prob_who_f_f = who_f_f / total_fake_cp\n",
        "\n",
        "    prob_t = total_true_cp / (total_true_cp + total_fake_cp)\n",
        "    prob_f = total_fake_cp / (total_true_cp + total_fake_cp)\n",
        "\n",
        "    # Bayesian network\n",
        "    a = BbnNode(Variable(0, 'Authenticity', ['1', '0']), [prob_t, prob_f])\n",
        "    who = BbnNode(Variable(1, 'Who', ['1', '0']), [prob_who_t_t, prob_who_f_t, prob_who_t_f, prob_who_f_f])\n",
        "\n",
        "    bbn = Bbn() \\\n",
        "        .add_node(a) \\\n",
        "        .add_node(who) \\\n",
        "        .add_edge(Edge(a, who, EdgeType.DIRECTED))\n",
        "\n",
        "    join_tree = InferenceController.apply(bbn)\n",
        "\n",
        "    # Testing\n",
        "    who_test = test_data[who_columns].copy()\n",
        "\n",
        "    who_test_attr = who_test.drop('majority_target', axis=1)\n",
        "\n",
        "    test_label = test_data['majority_target']\n",
        "\n",
        "    who_test_pred = who_model.predict(who_test_attr)\n",
        "\n",
        "    who_test_pred = who_test_pred.astype(int)\n",
        "\n",
        "    bn_predictions = []\n",
        "    for i in range(len(test_data)):\n",
        "        join_tree = InferenceController.apply(bbn)\n",
        "        who_ev = str(who_test_pred[i])\n",
        "\n",
        "        ev1 = EvidenceBuilder() \\\n",
        "            .with_node(join_tree.get_bbn_node_by_name('Who')) \\\n",
        "            .with_evidence(who_ev, 1.0) \\\n",
        "            .build()\n",
        "        join_tree.set_observation(ev1)\n",
        "\n",
        "        query_node = join_tree.get_bbn_node_by_name('Authenticity')\n",
        "        potential = join_tree.get_bbn_potential(query_node)\n",
        "\n",
        "        for node, posteriors in join_tree.get_posteriors().items():\n",
        "            if node == 'Authenticity':\n",
        "                if posteriors['1'] > posteriors['0']:\n",
        "                    predicted_label = 1\n",
        "                else:\n",
        "                    predicted_label = 0\n",
        "                bn_predictions.append(predicted_label)\n",
        "                break\n",
        "\n",
        "    bn_metrics['accuracy'].append(accuracy_score(test_label, bn_predictions))\n",
        "    bn_metrics['precision'].append(precision_score(test_label, bn_predictions))\n",
        "    bn_metrics['recall'].append(recall_score(test_label, bn_predictions))\n",
        "    bn_metrics['f1'].append(f1_score(test_label, bn_predictions))\n",
        "\n",
        "    print(f'BN model - Accuracy: {bn_metrics[\"accuracy\"][-1]}, Precision: {bn_metrics[\"precision\"][-1]}, Recall: {bn_metrics[\"recall\"][-1]}, F1: {bn_metrics[\"f1\"][-1]}')\n",
        "    print(\"\\n ----------------------------------------------------------------------------- \\n\")\n",
        "\n",
        "# Calculate and print average metrics for each model\n",
        "print(\"\\nAverage Metrics:\")\n",
        "for model_name, metrics in [('Who', who_metrics), ('BN', bn_metrics)]:\n",
        "    print(f'\\n{model_name} model:')\n",
        "    for metric_name, metric_values in metrics.items():\n",
        "        avg_metric = np.mean(metric_values)\n",
        "        print(f'  Average {metric_name}: {avg_metric:.4f}')"
      ],
      "metadata": {
        "id": "mWEYl5t6Na3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what module\n",
        "\n",
        "# Initialize lists to store metrics\n",
        "what_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "bn_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "for train_index, test_index in kf.split(data, data['majority_target']):\n",
        "    np.random.shuffle(train_index)\n",
        "    midpoint = len(train_index) // 2\n",
        "    module_training = train_index[:midpoint]\n",
        "    cpt_formation = train_index[midpoint:]\n",
        "    module_training_data = data.iloc[module_training].copy().reset_index(drop=True)\n",
        "    cpt_data = data.iloc[cpt_formation].copy().reset_index(drop=True)\n",
        "    test_data = data.iloc[test_index].copy().reset_index(drop=True)\n",
        "\n",
        "    # Module training\n",
        "    what_mt = module_training_data[what_columns].copy()\n",
        "\n",
        "    what_mt_attr = what_mt.drop('majority_target', axis=1)\n",
        "\n",
        "    what_mt_label = what_mt['majority_target']\n",
        "\n",
        "    # Training and evaluating the what model\n",
        "    X_train, X_test, y_train, y_test = train_test_split(what_mt_attr, what_mt_label, test_size=0.2, random_state=42)\n",
        "    what_model = RandomForestClassifier()\n",
        "    what_model.fit(X_train, y_train)\n",
        "    predictions = what_model.predict(X_test)\n",
        "    what_metrics['accuracy'].append(accuracy_score(y_test, predictions))\n",
        "    what_metrics['precision'].append(precision_score(y_test, predictions))\n",
        "    what_metrics['recall'].append(recall_score(y_test, predictions))\n",
        "    what_metrics['f1'].append(f1_score(y_test, predictions))\n",
        "    print(f'What model - Accuracy: {what_metrics[\"accuracy\"][-1]}, Precision: {what_metrics[\"precision\"][-1]}, Recall: {what_metrics[\"recall\"][-1]}, F1: {what_metrics[\"f1\"][-1]}')\n",
        "\n",
        "    # CPT formation\n",
        "    what_cp = cpt_data[what_columns].copy()\n",
        "\n",
        "    what_cp_attr = what_cp.drop('majority_target', axis=1)\n",
        "\n",
        "    what_cp_label = what_cp['majority_target']\n",
        "\n",
        "    what_cp_pred = what_model.predict(what_cp_attr)\n",
        "\n",
        "    total_true_cp = np.sum(cpt_data['majority_target'])\n",
        "    total_fake_cp = len(cpt_data) - np.sum(cpt_data['majority_target'])\n",
        "\n",
        "    what_t_t = what_t_f = what_f_t = what_f_f = 0\n",
        "\n",
        "    for i in range(len(cpt_data)):\n",
        "        if what_cp_pred[i] == 1 and what_cp_label[i] == 1:\n",
        "            what_t_t += 1\n",
        "        if what_cp_pred[i] == 1 and what_cp_label[i] == 0:\n",
        "            what_t_f += 1\n",
        "        if what_cp_pred[i] == 0 and what_cp_label[i] == 1:\n",
        "            what_f_t += 1\n",
        "        if what_cp_pred[i] == 0 and what_cp_label[i] == 0:\n",
        "            what_f_f += 1\n",
        "\n",
        "    prob_what_t_t = what_t_t / total_true_cp\n",
        "    prob_what_t_f = what_t_f / total_fake_cp\n",
        "    prob_what_f_t = what_f_t / total_true_cp\n",
        "    prob_what_f_f = what_f_f / total_fake_cp\n",
        "\n",
        "    prob_t = total_true_cp / (total_true_cp + total_fake_cp)\n",
        "    prob_f = total_fake_cp / (total_true_cp + total_fake_cp)\n",
        "\n",
        "    # Bayesian network\n",
        "    a = BbnNode(Variable(0, 'Authenticity', ['1', '0']), [prob_t, prob_f])\n",
        "    what = BbnNode(Variable(1, 'Who', ['1', '0']), [prob_what_t_t, prob_what_f_t, prob_what_t_f, prob_what_f_f])\n",
        "\n",
        "    bbn = Bbn() \\\n",
        "        .add_node(a) \\\n",
        "        .add_node(what) \\\n",
        "        .add_edge(Edge(a, what, EdgeType.DIRECTED))\n",
        "\n",
        "    join_tree = InferenceController.apply(bbn)\n",
        "\n",
        "    # Testing\n",
        "    what_test = test_data[what_columns].copy()\n",
        "\n",
        "    what_test_attr = what_test.drop('majority_target', axis=1)\n",
        "\n",
        "    test_label = test_data['majority_target']\n",
        "\n",
        "    what_test_pred = what_model.predict(what_test_attr)\n",
        "\n",
        "    what_test_pred = what_test_pred.astype(int)\n",
        "\n",
        "    bn_predictions = []\n",
        "    for i in range(len(test_data)):\n",
        "        join_tree = InferenceController.apply(bbn)\n",
        "        what_ev = str(what_test_pred[i])\n",
        "\n",
        "        ev1 = EvidenceBuilder() \\\n",
        "            .with_node(join_tree.get_bbn_node_by_name('Who')) \\\n",
        "            .with_evidence(what_ev, 1.0) \\\n",
        "            .build()\n",
        "        join_tree.set_observation(ev1)\n",
        "\n",
        "        query_node = join_tree.get_bbn_node_by_name('Authenticity')\n",
        "        potential = join_tree.get_bbn_potential(query_node)\n",
        "\n",
        "        for node, posteriors in join_tree.get_posteriors().items():\n",
        "            if node == 'Authenticity':\n",
        "                if posteriors['1'] > posteriors['0']:\n",
        "                    predicted_label = 1\n",
        "                else:\n",
        "                    predicted_label = 0\n",
        "                bn_predictions.append(predicted_label)\n",
        "                break\n",
        "\n",
        "    bn_metrics['accuracy'].append(accuracy_score(test_label, bn_predictions))\n",
        "    bn_metrics['precision'].append(precision_score(test_label, bn_predictions))\n",
        "    bn_metrics['recall'].append(recall_score(test_label, bn_predictions))\n",
        "    bn_metrics['f1'].append(f1_score(test_label, bn_predictions))\n",
        "\n",
        "    print(f'BN model - Accuracy: {bn_metrics[\"accuracy\"][-1]}, Precision: {bn_metrics[\"precision\"][-1]}, Recall: {bn_metrics[\"recall\"][-1]}, F1: {bn_metrics[\"f1\"][-1]}')\n",
        "    print(\"\\n ----------------------------------------------------------------------------- \\n\")\n",
        "\n",
        "# Calculate and print average metrics for each model\n",
        "print(\"\\nAverage Metrics:\")\n",
        "for model_name, metrics in [('What', what_metrics), ('BN', bn_metrics)]:\n",
        "    print(f'\\n{model_name} model:')\n",
        "    for metric_name, metric_values in metrics.items():\n",
        "        avg_metric = np.mean(metric_values)\n",
        "        print(f'  Average {metric_name}: {avg_metric:.4f}')"
      ],
      "metadata": {
        "id": "vX-QzDpdNGx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# when module\n",
        "\n",
        "# Initialize lists to store metrics\n",
        "when_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "bn_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "for train_index, test_index in kf.split(data, data['majority_target']):\n",
        "    np.random.shuffle(train_index)\n",
        "    midpoint = len(train_index) // 2\n",
        "    module_training = train_index[:midpoint]\n",
        "    cpt_formation = train_index[midpoint:]\n",
        "    module_training_data = data.iloc[module_training].copy().reset_index(drop=True)\n",
        "    cpt_data = data.iloc[cpt_formation].copy().reset_index(drop=True)\n",
        "    test_data = data.iloc[test_index].copy().reset_index(drop=True)\n",
        "\n",
        "    # Module training\n",
        "    when_mt = module_training_data[when_columns].copy()\n",
        "\n",
        "    when_mt_attr = when_mt.drop('majority_target', axis=1)\n",
        "\n",
        "    when_mt_label = when_mt['majority_target']\n",
        "\n",
        "    # Training and evaluating the when model\n",
        "    X_train, X_test, y_train, y_test = train_test_split(when_mt_attr, when_mt_label, test_size=0.2, random_state=42)\n",
        "    when_model = RandomForestClassifier()\n",
        "    when_model.fit(X_train, y_train)\n",
        "    predictions = when_model.predict(X_test)\n",
        "    when_metrics['accuracy'].append(accuracy_score(y_test, predictions))\n",
        "    when_metrics['precision'].append(precision_score(y_test, predictions))\n",
        "    when_metrics['recall'].append(recall_score(y_test, predictions))\n",
        "    when_metrics['f1'].append(f1_score(y_test, predictions))\n",
        "    print(f'When model - Accuracy: {when_metrics[\"accuracy\"][-1]}, Precision: {when_metrics[\"precision\"][-1]}, Recall: {when_metrics[\"recall\"][-1]}, F1: {when_metrics[\"f1\"][-1]}')\n",
        "\n",
        "    # CPT formation\n",
        "    when_cp = cpt_data[when_columns].copy()\n",
        "\n",
        "    when_cp_attr = when_cp.drop('majority_target', axis=1)\n",
        "\n",
        "    when_cp_label = when_cp['majority_target']\n",
        "\n",
        "    when_cp_pred = when_model.predict(when_cp_attr)\n",
        "\n",
        "    total_true_cp = np.sum(cpt_data['majority_target'])\n",
        "    total_fake_cp = len(cpt_data) - np.sum(cpt_data['majority_target'])\n",
        "\n",
        "    when_t_t = when_t_f = when_f_t = when_f_f = 0\n",
        "\n",
        "    for i in range(len(cpt_data)):\n",
        "        if when_cp_pred[i] == 1 and when_cp_label[i] == 1:\n",
        "            when_t_t += 1\n",
        "        if when_cp_pred[i] == 1 and when_cp_label[i] == 0:\n",
        "            when_t_f += 1\n",
        "        if when_cp_pred[i] == 0 and when_cp_label[i] == 1:\n",
        "            when_f_t += 1\n",
        "        if when_cp_pred[i] == 0 and when_cp_label[i] == 0:\n",
        "            when_f_f += 1\n",
        "\n",
        "    prob_when_t_t = when_t_t / total_true_cp\n",
        "    prob_when_t_f = when_t_f / total_fake_cp\n",
        "    prob_when_f_t = when_f_t / total_true_cp\n",
        "    prob_when_f_f = when_f_f / total_fake_cp\n",
        "\n",
        "    prob_t = total_true_cp / (total_true_cp + total_fake_cp)\n",
        "    prob_f = total_fake_cp / (total_true_cp + total_fake_cp)\n",
        "\n",
        "    # Bayesian network\n",
        "    a = BbnNode(Variable(0, 'Authenticity', ['1', '0']), [prob_t, prob_f])\n",
        "    when = BbnNode(Variable(1, 'Who', ['1', '0']), [prob_when_t_t, prob_when_f_t, prob_when_t_f, prob_when_f_f])\n",
        "\n",
        "    bbn = Bbn() \\\n",
        "        .add_node(a) \\\n",
        "        .add_node(when) \\\n",
        "        .add_edge(Edge(a, when, EdgeType.DIRECTED))\n",
        "\n",
        "    join_tree = InferenceController.apply(bbn)\n",
        "\n",
        "    # Testing\n",
        "    when_test = test_data[when_columns].copy()\n",
        "\n",
        "    when_test_attr = when_test.drop('majority_target', axis=1)\n",
        "\n",
        "    test_label = test_data['majority_target']\n",
        "\n",
        "    when_test_pred = when_model.predict(when_test_attr)\n",
        "\n",
        "    when_test_pred = when_test_pred.astype(int)\n",
        "\n",
        "    bn_predictions = []\n",
        "    for i in range(len(test_data)):\n",
        "        join_tree = InferenceController.apply(bbn)\n",
        "        when_ev = str(when_test_pred[i])\n",
        "\n",
        "        ev1 = EvidenceBuilder() \\\n",
        "            .with_node(join_tree.get_bbn_node_by_name('Who')) \\\n",
        "            .with_evidence(when_ev, 1.0) \\\n",
        "            .build()\n",
        "        join_tree.set_observation(ev1)\n",
        "\n",
        "        query_node = join_tree.get_bbn_node_by_name('Authenticity')\n",
        "        potential = join_tree.get_bbn_potential(query_node)\n",
        "\n",
        "        for node, posteriors in join_tree.get_posteriors().items():\n",
        "            if node == 'Authenticity':\n",
        "                if posteriors['1'] > posteriors['0']:\n",
        "                    predicted_label = 1\n",
        "                else:\n",
        "                    predicted_label = 0\n",
        "                bn_predictions.append(predicted_label)\n",
        "                break\n",
        "\n",
        "    bn_metrics['accuracy'].append(accuracy_score(test_label, bn_predictions))\n",
        "    bn_metrics['precision'].append(precision_score(test_label, bn_predictions))\n",
        "    bn_metrics['recall'].append(recall_score(test_label, bn_predictions))\n",
        "    bn_metrics['f1'].append(f1_score(test_label, bn_predictions))\n",
        "\n",
        "    print(f'BN model - Accuracy: {bn_metrics[\"accuracy\"][-1]}, Precision: {bn_metrics[\"precision\"][-1]}, Recall: {bn_metrics[\"recall\"][-1]}, F1: {bn_metrics[\"f1\"][-1]}')\n",
        "    print(\"\\n ----------------------------------------------------------------------------- \\n\")\n",
        "\n",
        "# Calculate and print average metrics for each model\n",
        "print(\"\\nAverage Metrics:\")\n",
        "for model_name, metrics in [('When', when_metrics), ('BN', bn_metrics)]:\n",
        "    print(f'\\n{model_name} model:')\n",
        "    for metric_name, metric_values in metrics.items():\n",
        "        avg_metric = np.mean(metric_values)\n",
        "        print(f'  Average {metric_name}: {avg_metric:.4f}')"
      ],
      "metadata": {
        "id": "4HFHxXXnNwD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# who (RF)\n",
        "\n",
        "# Initialize lists to store metrics\n",
        "who_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "for train_index, test_index in kf.split(data, data['majority_target']):\n",
        "    train_data = data.iloc[train_index].copy().reset_index(drop=True)\n",
        "    test_data = data.iloc[test_index].copy().reset_index(drop=True)\n",
        "\n",
        "    # Training the 'who' model\n",
        "    who_mt = train_data[who_columns].copy()\n",
        "    who_mt_attr = who_mt.drop('majority_target', axis=1)\n",
        "    who_mt_label = who_mt['majority_target']\n",
        "\n",
        "    who_model = RandomForestClassifier()\n",
        "    who_model.fit(who_mt_attr, who_mt_label)\n",
        "\n",
        "    # Evaluating the who model on the test set\n",
        "    who_test = test_data[who_columns].copy()\n",
        "    who_test_attr = who_test.drop('majority_target', axis=1)\n",
        "    test_label = test_data['majority_target']\n",
        "\n",
        "    predictions = who_model.predict(who_test_attr)\n",
        "\n",
        "    who_metrics['accuracy'].append(accuracy_score(test_label, predictions))\n",
        "    who_metrics['precision'].append(precision_score(test_label, predictions))\n",
        "    who_metrics['recall'].append(recall_score(test_label, predictions))\n",
        "    who_metrics['f1'].append(f1_score(test_label, predictions))\n",
        "\n",
        "    print(f'Who model - Accuracy: {who_metrics[\"accuracy\"][-1]}, Precision: {who_metrics[\"precision\"][-1]}, Recall: {who_metrics[\"recall\"][-1]}, F1: {who_metrics[\"f1\"][-1]}')\n",
        "    print(\"\\n ----------------------------------------------------------------------------- \\n\")\n",
        "\n",
        "# Calculate and print average metrics\n",
        "print(\"\\nAverage Metrics:\")\n",
        "for model_name, metrics in [('Who', who_metrics)]:\n",
        "    print(f'\\n{model_name} model:')\n",
        "    for metric_name, metric_values in metrics.items():\n",
        "        avg_metric = np.mean(metric_values)\n",
        "        print(f'  Average {metric_name}: {avg_metric:.4f}')\n"
      ],
      "metadata": {
        "id": "oybT1d__gh_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# when (RF)\n",
        "\n",
        "# Initialize lists to store metrics\n",
        "when_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "for train_index, test_index in kf.split(data, data['majority_target']):\n",
        "    train_data = data.iloc[train_index].copy().reset_index(drop=True)\n",
        "    test_data = data.iloc[test_index].copy().reset_index(drop=True)\n",
        "\n",
        "    # Training the 'when' model\n",
        "    when_mt = train_data[when_columns].copy()\n",
        "    when_mt_attr = when_mt.drop('majority_target', axis=1)\n",
        "    when_mt_label = when_mt['majority_target']\n",
        "\n",
        "    when_model = RandomForestClassifier()\n",
        "    when_model.fit(when_mt_attr, when_mt_label)\n",
        "\n",
        "    # Evaluating the when model on the test set\n",
        "    when_test = test_data[when_columns].copy()\n",
        "    when_test_attr = when_test.drop('majority_target', axis=1)\n",
        "    test_label = test_data['majority_target']\n",
        "\n",
        "    predictions = when_model.predict(when_test_attr)\n",
        "\n",
        "    when_metrics['accuracy'].append(accuracy_score(test_label, predictions))\n",
        "    when_metrics['precision'].append(precision_score(test_label, predictions))\n",
        "    when_metrics['recall'].append(recall_score(test_label, predictions))\n",
        "    when_metrics['f1'].append(f1_score(test_label, predictions))\n",
        "\n",
        "    print(f'When model - Accuracy: {when_metrics[\"accuracy\"][-1]}, Precision: {when_metrics[\"precision\"][-1]}, Recall: {when_metrics[\"recall\"][-1]}, F1: {when_metrics[\"f1\"][-1]}')\n",
        "    print(\"\\n ----------------------------------------------------------------------------- \\n\")\n",
        "\n",
        "# Calculate and print average metrics\n",
        "print(\"\\nAverage Metrics:\")\n",
        "for model_name, metrics in [('When', when_metrics)]:\n",
        "    print(f'\\n{model_name} model:')\n",
        "    for metric_name, metric_values in metrics.items():\n",
        "        avg_metric = np.mean(metric_values)\n",
        "        print(f'  Average {metric_name}: {avg_metric:.4f}')\n"
      ],
      "metadata": {
        "id": "bm_PGJ2Xghss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what (RF)\n",
        "\n",
        "# Initialize lists to store metrics\n",
        "what_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "for train_index, test_index in kf.split(data, data['majority_target']):\n",
        "    train_data = data.iloc[train_index].copy().reset_index(drop=True)\n",
        "    test_data = data.iloc[test_index].copy().reset_index(drop=True)\n",
        "\n",
        "    # Training the 'what' model\n",
        "    what_mt = train_data[what_columns].copy()\n",
        "    what_mt_attr = what_mt.drop('majority_target', axis=1)\n",
        "    what_mt_label = what_mt['majority_target']\n",
        "\n",
        "    what_model = RandomForestClassifier()\n",
        "    what_model.fit(what_mt_attr, what_mt_label)\n",
        "\n",
        "    # Evaluating the what model on the test set\n",
        "    what_test = test_data[what_columns].copy()\n",
        "    what_test_attr = what_test.drop('majority_target', axis=1)\n",
        "    test_label = test_data['majority_target']\n",
        "\n",
        "    predictions = what_model.predict(what_test_attr)\n",
        "\n",
        "    what_metrics['accuracy'].append(accuracy_score(test_label, predictions))\n",
        "    what_metrics['precision'].append(precision_score(test_label, predictions))\n",
        "    what_metrics['recall'].append(recall_score(test_label, predictions))\n",
        "    what_metrics['f1'].append(f1_score(test_label, predictions))\n",
        "\n",
        "    print(f'What model - Accuracy: {what_metrics[\"accuracy\"][-1]}, Precision: {what_metrics[\"precision\"][-1]}, Recall: {what_metrics[\"recall\"][-1]}, F1: {what_metrics[\"f1\"][-1]}')\n",
        "    print(\"\\n ----------------------------------------------------------------------------- \\n\")\n",
        "\n",
        "# Calculate and print average metrics\n",
        "print(\"\\nAverage Metrics:\")\n",
        "for model_name, metrics in [('What', what_metrics)]:\n",
        "    print(f'\\n{model_name} model:')\n",
        "    for metric_name, metric_values in metrics.items():\n",
        "        avg_metric = np.mean(metric_values)\n",
        "        print(f'  Average {metric_name}: {avg_metric:.4f}')\n"
      ],
      "metadata": {
        "id": "vAqhIXVvgh4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cjxn7dqClaX"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7fnNIbiaaeu9",
        "BJoqozB8fAYq",
        "B4WBKtapCwyU"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}